# Real-time Pipeline: Kafka + AWS + Snowflake 

**Real-time data pipeline** do przetwarzania danych o cenach kryptowalut. Dane sÄ… pobierane z API CoinGecko, przesyÅ‚ane przez Apache Kafka, zapisywane do AWS S3 w warstwach (bronze, silver, gold), a nastÄ™pnie Å‚adowane do hurtowni danych Snowflake z modelowaniem wymiarowym (dimensional modeling).

---

## ðŸ›  Stack Technologiczny

| Kategoria       | Technologie                          |
|-----------------|--------------------------------------|
| **Streaming**   | Apache Kafka, Python (kafka-python) |
| **Dane wejÅ›ciowe** | CoinGecko API (REST)               |
| **Przechowywanie** | AWS S3 (buckety: bronze, silver, gold, glue) |
| **ETL**         | AWS Glue (PySpark)                  |
| **Orkiestracja** | AWS Lambda, CloudFormation, S3 Events |
| **Hurtownia**   | Snowflake (Snowpipe, Stage, Warehouse) |
| **Konteneryzacja** | Docker, docker-compose             |
| **Testy & CI/CD** | pytest, flake8, black, GitHub Actions |
| **JÄ™zyk**       | Python 3.11                         |

---

**ðŸš€Opis projektu:**

- **Streaming danych**: Producent (producer) pobiera dane co 10 sekund i wysyÅ‚a do Kafki.
- **Przetwarzanie ETL**: Konsument zapisuje surowe dane do S3 (bronze). Glue jobs czyÅ›ci i transformujÄ… dane (silver â†’ gold).
- **Warstwy danych**: Bronze (surowe JSON), Silver (oczyszczone Parquet), Gold (modelowanie wymiarowe: dim_coin + fact_market_metrics).
- **Automatyzacja**: Lambda triggery na S3 uruchamiajÄ… Glue jobs automatycznie.
- **Hurtownia danych**: Snowflake z automatycznym Å‚adowaniem (Snowpipe) z S3.
- **CI/CD**: GitHub Actions do testÃ³w i lintingu.

---

## Wymagania

- **Lokalne:**
  - Python 3.11+
  - Docker & Docker Compose
  - AWS CLI (z konfiguracjÄ…: `aws configure`)

- **AWS (Free Tier):**
  - Konto AWS (eu-north-1)
  - IAM Role: `glue_s3_role` z politykami `glue_s3_policy` i `snowflake_s3_policy`
  - Buckety S3: `kafka-realtime-crypto-bronze`, `kafka-realtime-crypto-silver`, `kafka-realtime-crypto-gold`, `kafka-realtime-crypto-glue`
  - Glue Jobs: `bronze-to-silver-job`, `silver-to-gold-job`
  - Lambda Functions: `s3-bronze-trigger-glue`, `s3-silver-trigger-glue`
  - CloudFormation Stacks: `bronze-glue-trigger-stack`, `silver-glue-trigger-stack`

- **Snowflake:**
  - Konto Snowflake (trial/free)
  - Warehouse: `COMPUTE_WH` (XSMALL)
  - Database: `crypto_warehouse`
  - Tabele: `dim_coin`, `fact_market_metrics`
  - Storage Integration: `s3_crypto_integration`
  - Stage: `gold_stage`
  - Pipes: `dim_coin_pipe`, `fact_market_metrics_pipe`

## Instalacja

1. **Sklonuj repozytorium:**

```bash
git clone https://github.com/tomsongracz/kafka-realtime-crypto-pipeline.git
   cd kafka-realtime-crypto-pipeline
```


2. **Zainstaluj zaleÅ¼noÅ›ci Python:**

```bash
pip install -r requirements.txt
```

3. **Skonfiguruj AWS CLI:**

```bash
aws configure
```

Wpisz Access Key ID, Secret Access Key, Region: `eu-north-1`.

4. **UtwÃ³rz buckety S3 (jeÅ›li nie istniejÄ…):**

```bash
   aws s3 mb s3://kafka-realtime-crypto-bronze --region eu-north-1
   aws s3 mb s3://kafka-realtime-crypto-silver --region eu-north-1
   aws s3 mb s3://kafka-realtime-crypto-gold --region eu-north-1
   aws s3 mb s3://kafka-realtime-crypto-glue --region eu-north-1
```


5. **Wygeneruj i przypisz polityki IAM (uÅ¼yj plikÃ³w JSON z `aws/`):**
- UtwÃ³rz politykÄ™: `aws iam create-policy --policy-name glue_s3_policy --policy-document file://aws/glue_s3_policy.json`
- UtwÃ³rz rolÄ™: `aws iam create-role --role-name glue_s3_role --assume-role-policy-document file://aws/trust_policy.json`
- Przypisz politykÄ™: `aws iam attach-role-policy --role-name glue_s3_role --policy-arn arn:aws:iam::YOUR_ACCOUNT:policy/glue_s3_policy`

6. **WrzuÄ‡ skrypty Glue do S3:**

```bash
   aws s3 cp aws/spark/bronze_to_silver/bronze_to_silver_glue.py s3://kafka-realtime-crypto-glue/
   aws s3 cp aws/spark/silver_to_gold/silver_to_gold_glue.py s3://kafka-realtime-crypto-glue/
```


7. **UtwÃ³rz Glue Jobs:**

```bash
   aws glue create-job --region eu-north-1 --name bronze-to-silver-job --role arn:aws:iam::YOUR_ACCOUNT:role/glue_s3_role --command "Name=glueetl,ScriptLocation=s3://kafka-realtime-crypto-glue/bronze_to_silver_glue.py,PythonVersion=3" --default-arguments '{"--job-language":"python","--bronze_bucket":"kafka-realtime-crypto-bronze","--silver_bucket":"kafka-realtime-crypto-silver","--enable-continuous-cloudwatch-log":"true"}' --glue-version "4.0" --number-of-workers 2 --worker-type G.1X   aws glue create-job --region eu-north-1 --name silver-to-gold-job --role arn:aws:iam::YOUR_ACCOUNT:role/glue_s3_role --command "Name=glueetl,ScriptLocation=s3://kafka-realtime-crypto-glue/silver_to_gold_glue.py,PythonVersion=3" --default-arguments '{"--job-language":"python","--silver_bucket":"kafka-realtime-crypto-silver","--gold_bucket":"kafka-realtime-crypto-gold","--enable-continuous-cloudwatch-log":"true"}' --glue-version "4.0" --number-of-workers 2 --worker-type G.1X
```

8. **WdrÃ³Å¼ triggery Lambda (CloudFormation):**

```bash
   aws cloudformation deploy --template-file aws/lambda_bronze_trigger.yaml --stack-name bronze-glue-trigger-stack --capabilities CAPABILITY_NAMED_IAM --region eu-north-1
   aws cloudformation deploy --template-file aws/lambda_silver_trigger.yaml --stack-name silver-glue-trigger-stack --capabilities CAPABILITY_NAMED_IAM --region eu-north-1
```

9. **Skonfiguruj S3 Notifications (dla Lambda):**

```bash
   aws s3api put-bucket-notification-configuration --bucket kafka-realtime-crypto-bronze --notification-configuration '{"LambdaFunctionConfigurations":[{"LambdaFunctionArn":"arn:aws:lambda:eu-north-1:YOUR_ACCOUNT:function:s3-bronze-trigger-glue","Events":["s3:ObjectCreated:*"],"Filter":{"Key":{"FilterRules":[{"Name":"prefix","Value":"bronze/"}]}}]}' --region eu-north-1   aws s3api put-bucket-notification-configuration --bucket kafka-realtime-crypto-silver --notification-configuration '{"LambdaFunctionConfigurations":[{"LambdaFunctionArn":"arn:aws:lambda:eu-north-1:YOUR_ACCOUNT:function:s3-silver-trigger-glue","Events":["s3:ObjectCreated:*"],"Filter":{"Key":{"FilterRules":[{"Name":"prefix","Value":"silver/"}]}}]}' --region eu-north-1
```

10. **Skonfiguruj Snowflake:**
 - UtwÃ³rz Warehouse, Database, Tabele, Stage, Pipes (patrz SQL w repozytorium lub dokumentacji Snowflake).
 - UtwÃ³rz Storage Integration i SQS Queue dla Snowpipe (zobacz `snowflake_s3_policy.json`).

## Uruchomienie Lokalne

1. **Uruchom Kafka + Zookeeper (Docker):**

```bash
 ./start-local.sh
```

SprawdÅº: `docker ps` (powinny dziaÅ‚aÄ‡ `zookeeper` i `kafka`).

2. **Uruchom Producer (pobiera dane z CoinGecko):**

```bash
python kafka/producer/producer.py
```

Widoczny output: `Sent: {'name': 'Bitcoin', ...}` co 10s.

3. **W innym terminalu: Uruchom Consumer (zapis do S3 bronze):**

```bash
python kafka/consumer/consumer.py
```

Widoczny output: `Saved to S3: bronze/btc/2025-10-30_12-00-00.json`.

4. **RÄ™czne uruchomienie Glue Jobs (testowo):**

```bash
   aws glue start-job-run --job-name bronze-to-silver-job --region eu-north-1
   aws glue start-job-run --job-name silver-to-gold-job --region eu-north-1
```

5. **SprawdÅº dane w Snowflake:**

```sql
   SELECT * FROM crypto_warehouse.public.dim_coin LIMIT 5;
   SELECT * FROM crypto_warehouse.public.fact_market_metrics LIMIT 5;
```

## Testy

Uruchom testy jednostkowe i integracyjne:

```bash
./dev_tools.sh test
```
Lub bezpoÅ›rednio:

```bash
pytest tests/ -v
```

- `test_producer.py`: Mock API CoinGecko, sprawdza serializacjÄ™ Kafka.
- `test_consumer.py`: Mock S3, sprawdza zapis JSON.
- `test_spark_jobs.py`: Symulacja PySpark na Pandas, testy ETL i dimensional modeling.

Linting i formatowanie:

```bash
./dev_tools.sh lint
```

## CI/CD

- **GitHub Actions**: Automatyczne testy + linting na push/PR do `main` (patrz `.github/workflows/ci.yml`).
- **Deployment**: RÄ™czne via AWS CLI lub GitHub Actions (moÅ¼na rozszerzyÄ‡ o CD).

## Struktura Projektu

```bash
kafka-realtime-crypto-pipeline/
â”‚
â”œâ”€â”€ aws/                                      # Komponenty chmurowe (Spark jobs, konfiguracje AWS)
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”œâ”€â”€ bronze_to_silver/                 # ETL: przetwarzanie danych z warstwy bronze â†’ silver
â”‚   â”‚   â”‚   â”œâ”€â”€ bronze_to_silver_glue.py      # Kod Spark (PySpark / Glue): czyszczenie, walidacja i standaryzacja danych
â”‚   â”‚   â”‚   â””â”€â”€ job_config.json               # Konfiguracja Glue Job (parametry, rola, lokalizacja skryptu)
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ silver_to_gold/                   # ETL: przetwarzanie danych z warstwy silver â†’ gold
â”‚   â”‚       â”œâ”€â”€ silver_to_gold_glue.py        # Kod Spark (PySpark / Glue): modelowanie wymiarowe (fact + dim)
â”‚   â”‚       â””â”€â”€ job_config.json               # Konfiguracja Glue Job dla silver â†’ gold
â”‚   â”‚
â”‚   â”œâ”€â”€ trust_policy.json                     # CzÄ™Å›Ä‡ IAM Role
â”‚   â”œâ”€â”€ snowflake_s3_policy.json              # Czytanie Snowflake z S3
â”‚   â””â”€â”€ glue_s3_policy.json                   # Polityka IAM nadajÄ…ca Glue Jobowi dostÄ™p do bucketÃ³w S3 i CloudWatch
â”‚
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer/
â”‚   â”‚   â””â”€â”€ producer.py                       # Skrypt producenta: pobiera dane z CoinGecko i wysyÅ‚a do Apache Kafka
â”‚   â”œâ”€â”€ consumer/
â”‚   â”‚   â””â”€â”€ consumer.py                       # Skrypt konsumenta: odbiera dane z Kafki i zapisuje do AWS S3 (bronze)
â”‚   â””â”€â”€ docker-compose.yml                    # Konfiguracja Kafki i Zookeepera w Å›rodowisku lokalnym
â”‚
â”œâ”€â”€ tests/                                    # Testy jednostkowe i integracyjne
â”‚   â”œâ”€â”€ test_producer.py                      # Testy dla producer.py
â”‚   â”œâ”€â”€ test_consumer.py                      # Testy dla consumer.py
â”‚   â””â”€â”€ test_spark_jobs.py                    # Testy dla jobÃ³w Spark (mockowane)
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/                    
â”‚       â””â”€â”€ ci.yaml                           # CI
â”‚
â”œâ”€â”€ .gitignore                                # Plik ignorowania Git
â”œâ”€â”€ requirements.txt                          # Lista zaleÅ¼noÅ›ci Pythona
â”œâ”€â”€ lambda_bronze_trigger.yaml                # Szablon CloudFormation (Lambda trigger na bronze)
â”œâ”€â”€ lambda_silver_trigger.yaml                # Szablon CloudFormation (Lambda trigger na silver)
â”œâ”€â”€ dev_tools.sh                              # Skrypt umoÅ¼liwiajÄ…cy Å‚atwe testy i linting 
â”œâ”€â”€ start-local.sh                            # Skrypt restartujÄ…cy kontenery z opcjÄ… buildu obrazu
â””â”€â”€ README.md                                 # Dokumentacja projektu
```

## Rozszerzenia i przemyÅ›lenia

- MoÅ¼na dodaÄ‡ wiÄ™cej kryptowalut lub ÅºrÃ³deÅ‚ API.
- Implementacja monitoringu (CloudWatch Alerts).
- Dodanie dashboardu (np. QuickSight na Snowflake).

## ðŸ‘¤ Autor
Projekt przygotowany w celach edukacyjnych i demonstracyjnych.
MoÅ¼esz mnie znaleÅºÄ‡ na GitHubie: [tomsongracz](https://github.com/tomsongracz)







